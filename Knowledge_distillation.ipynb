{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ad50e11c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ae0f43434fb485793437984273e50fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b56d4044",
   "metadata": {},
   "outputs": [],
   "source": [
    "student_id = \"google/bert_uncased_L-4_H-256_A-4\"\n",
    "teacher_id = \"agvidit1/Bert_TG-HS-HX_pretrain\"\n",
    "\n",
    "# name for our repository on the hub\n",
    "repo_name = \"tinybert-TG-HS-HX-parentpretrained\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "648ed176",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12a49106a5e84d04b40c73151887f7ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cca0ce18af2148fa9a98de8171798dd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/383 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers import BertTokenizer, DistilBertTokenizer\n",
    "\n",
    "# init tokenizer\n",
    "teacher_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "student_tokenizer = BertTokenizer.from_pretrained(student_id)\n",
    "\n",
    "# sample input\n",
    "sample = \"This is a basic example, with different words to test.\"\n",
    "\n",
    "# assert results\n",
    "assert teacher_tokenizer(sample) == student_tokenizer(sample), \"Tokenizers haven't created the same output\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d1bc2eed-68bd-4df9-a6c4-36eb6f88b970",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_id=\"glue\"\n",
    "# dataset_config=\"sst2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "51dc185c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cec329a6899948ac95844d52e80791cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/353 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a45b89d2cafc4266b08506cd7160608f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f0dff18fd50474f9868e6f87b7f421c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/2.88M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e9256ec340545e586f0ea9feed71504",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b6ecc09c9ec4e128daa78e4c1e12b2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/35982 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "dataset = load_dataset(\"agvidit1/Dataset-TG-HS-HX-Processed\")\n",
    "columns_to_remove = ['__index_level_0__']\n",
    "dataset = dataset.remove_columns(columns_to_remove)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7a855bd8-85a2-4bfb-8449-c92d794e5719",
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled_dataset = dataset.shuffle(seed=42)\n",
    "total_rows = len(shuffled_dataset['train'])\n",
    "train_size = int(total_rows * 0.70)\n",
    "validation_size = int(total_rows * 0.20)\n",
    "test_size = total_rows - train_size - validation_size\n",
    "train_dataset = shuffled_dataset['train'].select(range(train_size))\n",
    "validation_dataset = shuffled_dataset['train'].select(range(train_size, train_size + validation_size))\n",
    "test_dataset = shuffled_dataset['train'].select(range(train_size + validation_size, total_rows))\n",
    "\n",
    "split_dataset = {\n",
    "    'train': train_dataset,\n",
    "    'validation': validation_dataset,\n",
    "    'test': test_dataset\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "73d2951e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1e346093",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a7355aad394466fa17fea3ec945edc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25187 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "214eca759f8444d0973d064069374840",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7196 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1f51d88e41948a78ebb0ddd7ddce3b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3599 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'train': Dataset({\n",
       "     features: ['text', 'labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "     num_rows: 25187\n",
       " }),\n",
       " 'validation': Dataset({\n",
       "     features: ['text', 'labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "     num_rows: 7196\n",
       " }),\n",
       " 'test': Dataset({\n",
       "     features: ['text', 'labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "     num_rows: 3599\n",
       " })}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def process(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"text\"], truncation=True, max_length=256\n",
    "    )\n",
    "    return tokenized_inputs\n",
    "\n",
    "\n",
    "tokenized_datasets = {split: split_dataset[split].map(process, batched=True) for split in split_dataset}\n",
    "tokenized_datasets = {split: dataset.rename_column(\"label\", \"labels\") for split, dataset in tokenized_datasets.items()}\n",
    "# tokenized_datasets = {split: dataset.rename_column(\"user_id\", \"idx\") for split, dataset in tokenized_datasets.items()}\n",
    "\n",
    "tokenized_datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "030d068d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class DistillationTrainingArguments(TrainingArguments):\n",
    "    def __init__(self, *args, alpha=0.5, temperature=2.0, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        self.alpha = alpha\n",
    "        self.temperature = temperature\n",
    "\n",
    "class DistillationTrainer(Trainer):\n",
    "    def __init__(self, *args, teacher_model=None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.teacher = teacher_model\n",
    "        # place teacher on same device as student\n",
    "        self._move_model_to_device(self.teacher,self.model.device)\n",
    "        self.teacher.eval()\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "\n",
    "        # compute student output\n",
    "        outputs_student = model(**inputs)\n",
    "        student_loss=outputs_student.loss\n",
    "        # compute teacher output\n",
    "        with torch.no_grad():\n",
    "          outputs_teacher = self.teacher(**inputs)\n",
    "\n",
    "        # assert size\n",
    "        assert outputs_student.logits.size() == outputs_teacher.logits.size()\n",
    "\n",
    "        # Soften probabilities and compute distillation loss\n",
    "        loss_function = nn.KLDivLoss(reduction=\"batchmean\")\n",
    "        loss_logits = (loss_function(\n",
    "            F.log_softmax(outputs_student.logits / self.args.temperature, dim=-1),\n",
    "            F.softmax(outputs_teacher.logits / self.args.temperature, dim=-1)) * (self.args.temperature ** 2))\n",
    "        # Return weighted student loss\n",
    "        loss = self.args.alpha * student_loss + (1. - self.args.alpha) * loss_logits\n",
    "        return (loss, outputs_student) if return_outputs else loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fe62acf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed1ac6a2a1d745db85a8ed12938d3678",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at agvidit1/Bert_TG-HS-HX_pretrain and are newly initialized: ['classifier.weight', 'bert.pooler.dense.bias', 'classifier.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77c706808b0244ef806a999e817f52a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/45.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-4_H-256_A-4 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, DataCollatorWithPadding, BertForSequenceClassification\n",
    "from huggingface_hub import HfFolder\n",
    "\n",
    "# create label2id, id2label dicts for nice outputs for the model\n",
    "labels = list(set(tokenized_datasets[\"train\"][\"labels\"]))\n",
    "num_labels = len(labels)\n",
    "label2id = {label: idx for idx, label in enumerate(labels)}\n",
    "id2label = {idx: label for label, idx in label2id.items()}\n",
    "# for i, label in enumerate(labels):\n",
    "#     label2id[label] = str(i)\n",
    "#     id2label[str(i)] = label\n",
    "\n",
    "# define training args\n",
    "training_args = DistillationTrainingArguments(\n",
    "    output_dir=repo_name,\n",
    "    num_train_epochs=7,\n",
    "    per_device_train_batch_size=128,\n",
    "    per_device_eval_batch_size=128,\n",
    "    learning_rate=6e-5,\n",
    "    seed=33,\n",
    "    # logging & evaluation strategies\n",
    "    logging_dir=f\"{repo_name}/logs\",\n",
    "    logging_strategy=\"epoch\", # to get more information to TB\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    report_to=\"tensorboard\",\n",
    "    # push to hub parameters\n",
    "    push_to_hub=True,\n",
    "    hub_strategy=\"every_save\",\n",
    "    hub_model_id=repo_name,\n",
    "    hub_token=HfFolder.get_token(),\n",
    "    # distilation parameters\n",
    "    alpha=0.5,\n",
    "    temperature=4.0\n",
    "    )\n",
    "\n",
    "# define data_collator\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# define model\n",
    "teacher_model = BertForSequenceClassification.from_pretrained(\n",
    "    teacher_id,\n",
    "    ignore_mismatched_sizes=True,\n",
    "    num_labels=num_labels,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")\n",
    "\n",
    "# define student model\n",
    "student_model = BertForSequenceClassification.from_pretrained(\n",
    "    student_id,\n",
    "    num_labels=num_labels,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e0f70d91-1b1b-499b-b8f7-63d60b7966c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "import numpy as np\n",
    "\n",
    "# define metrics and metrics function\n",
    "accuracy_metric = load_metric( \"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    acc = accuracy_metric.compute(predictions=predictions, references=labels)\n",
    "    return {\n",
    "        \"accuracy\": acc[\"accuracy\"],\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9b0a247f-965c-41cd-a53b-9003caf84929",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = DistillationTrainer(\n",
    "    student_model,\n",
    "    training_args,\n",
    "    teacher_model=teacher_model,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3638bde2-7dc9-4529-8638-bcbc41265d7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bf3cf211-90f2-40b9-b4b4-bf2f6fad5484",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1379' max='1379' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1379/1379 53:56, Epoch 7/7]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.301700</td>\n",
       "      <td>0.288796</td>\n",
       "      <td>0.793218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.285900</td>\n",
       "      <td>0.281244</td>\n",
       "      <td>0.808644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.277200</td>\n",
       "      <td>0.281716</td>\n",
       "      <td>0.817260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.270700</td>\n",
       "      <td>0.279905</td>\n",
       "      <td>0.823096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.265200</td>\n",
       "      <td>0.280279</td>\n",
       "      <td>0.824347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.259800</td>\n",
       "      <td>0.281970</td>\n",
       "      <td>0.825598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.257800</td>\n",
       "      <td>0.281981</td>\n",
       "      <td>0.825042</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1379, training_loss=0.2740491738433506, metrics={'train_runtime': 3237.4974, 'train_samples_per_second': 54.458, 'train_steps_per_second': 0.426, 'total_flos': 309665393535204.0, 'train_loss': 0.2740491738433506, 'epoch': 7.0})"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1fe6069d-c103-4ef3-86fe-69f1f0d43b7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: optuna in c:\\users\\airjo\\anaconda3\\envs\\distilbert\\lib\\site-packages (3.4.0)\n",
      "Requirement already satisfied: alembic>=1.5.0 in c:\\users\\airjo\\anaconda3\\envs\\distilbert\\lib\\site-packages (from optuna) (1.13.0)\n",
      "Requirement already satisfied: colorlog in c:\\users\\airjo\\anaconda3\\envs\\distilbert\\lib\\site-packages (from optuna) (6.8.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\airjo\\anaconda3\\envs\\distilbert\\lib\\site-packages (from optuna) (1.26.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\airjo\\anaconda3\\envs\\distilbert\\lib\\site-packages (from optuna) (23.1)\n",
      "Requirement already satisfied: sqlalchemy>=1.3.0 in c:\\users\\airjo\\anaconda3\\envs\\distilbert\\lib\\site-packages (from optuna) (2.0.23)\n",
      "Requirement already satisfied: tqdm in c:\\users\\airjo\\anaconda3\\envs\\distilbert\\lib\\site-packages (from optuna) (4.66.1)\n",
      "Requirement already satisfied: PyYAML in c:\\users\\airjo\\anaconda3\\envs\\distilbert\\lib\\site-packages (from optuna) (6.0.1)\n",
      "Requirement already satisfied: Mako in c:\\users\\airjo\\anaconda3\\envs\\distilbert\\lib\\site-packages (from alembic>=1.5.0->optuna) (1.3.0)\n",
      "Requirement already satisfied: typing-extensions>=4 in c:\\users\\airjo\\anaconda3\\envs\\distilbert\\lib\\site-packages (from alembic>=1.5.0->optuna) (4.7.1)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\airjo\\anaconda3\\envs\\distilbert\\lib\\site-packages (from sqlalchemy>=1.3.0->optuna) (3.0.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\airjo\\anaconda3\\envs\\distilbert\\lib\\site-packages (from colorlog->optuna) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in c:\\users\\airjo\\anaconda3\\envs\\distilbert\\lib\\site-packages (from Mako->alembic>=1.5.0->optuna) (2.1.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f01b067a-f5cd-4d07-85bb-5701e7c5701a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hp_space(trial):\n",
    "    return {\n",
    "        \"num_train_epochs\": trial.suggest_int(\"num_train_epochs\", 3, 8),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-6, 1e-4 ,log=True),\n",
    "        \"alpha\": trial.suggest_float(\"alpha\", 0, 1),\n",
    "        \"temperature\": trial.suggest_int(\"temperature\", 2, 30),\n",
    "        # \"per_device_train_batch_size\": trial.suggest_categorical(\"per_device_train_batch_size\", [16,32]),\n",
    "        # \"per_device_eval_batch_size\": trial.suggest_categorical(\"per_device_eval_batch_size\", [16,32]),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a6b9fa6c-a06b-4df4-a340-1ca07f403a56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-4_H-256_A-4 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "[I 2023-12-10 21:44:57,714] A new study created in memory with name: no-name-dea694e1-f36f-42c3-afb7-e28595695974\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-4_H-256_A-4 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1182' max='1182' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1182/1182 47:10, Epoch 6/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.612700</td>\n",
       "      <td>0.562056</td>\n",
       "      <td>0.725820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.556200</td>\n",
       "      <td>0.517233</td>\n",
       "      <td>0.752362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.529700</td>\n",
       "      <td>0.498158</td>\n",
       "      <td>0.760145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.518900</td>\n",
       "      <td>0.488543</td>\n",
       "      <td>0.765981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.511700</td>\n",
       "      <td>0.483972</td>\n",
       "      <td>0.767927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.506300</td>\n",
       "      <td>0.482752</td>\n",
       "      <td>0.767510</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-10 22:32:10,610] Trial 0 finished with value: 0.7675097276264592 and parameters: {'num_train_epochs': 6, 'learning_rate': 3.0069416525035787e-06, 'alpha': 0.9772734766942756, 'temperature': 16}. Best is trial 0 with value: 0.7675097276264592.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-4_H-256_A-4 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='985' max='985' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [985/985 39:17, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.282500</td>\n",
       "      <td>0.269023</td>\n",
       "      <td>0.752362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.272400</td>\n",
       "      <td>0.265089</td>\n",
       "      <td>0.771818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.269200</td>\n",
       "      <td>0.263475</td>\n",
       "      <td>0.775987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.268100</td>\n",
       "      <td>0.262498</td>\n",
       "      <td>0.778071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.267400</td>\n",
       "      <td>0.262347</td>\n",
       "      <td>0.779461</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-10 23:11:39,311] Trial 1 finished with value: 0.7794608115619789 and parameters: {'num_train_epochs': 5, 'learning_rate': 5.7546531738235265e-06, 'alpha': 0.4394092153370026, 'temperature': 23}. Best is trial 1 with value: 0.7794608115619789.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-4_H-256_A-4 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='985' max='985' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [985/985 39:02, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.564000</td>\n",
       "      <td>0.532214</td>\n",
       "      <td>0.698999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.526100</td>\n",
       "      <td>0.496853</td>\n",
       "      <td>0.738883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.505200</td>\n",
       "      <td>0.481561</td>\n",
       "      <td>0.748888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.497500</td>\n",
       "      <td>0.474243</td>\n",
       "      <td>0.754864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.493500</td>\n",
       "      <td>0.472753</td>\n",
       "      <td>0.754308</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-10 23:50:44,718] Trial 2 finished with value: 0.754307948860478 and parameters: {'num_train_epochs': 5, 'learning_rate': 2.09805683059569e-06, 'alpha': 0.8786599620319323, 'temperature': 9}. Best is trial 1 with value: 0.7794608115619789.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-4_H-256_A-4 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1379' max='1379' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1379/1379 54:21, Epoch 7/7]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.200200</td>\n",
       "      <td>0.194488</td>\n",
       "      <td>0.796693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.194600</td>\n",
       "      <td>0.191451</td>\n",
       "      <td>0.808644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.191200</td>\n",
       "      <td>0.191405</td>\n",
       "      <td>0.815314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.189000</td>\n",
       "      <td>0.190453</td>\n",
       "      <td>0.818788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.187100</td>\n",
       "      <td>0.190758</td>\n",
       "      <td>0.824625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.185300</td>\n",
       "      <td>0.190897</td>\n",
       "      <td>0.824625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.184300</td>\n",
       "      <td>0.190875</td>\n",
       "      <td>0.823235</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-11 00:45:18,342] Trial 3 finished with value: 0.8232351306281267 and parameters: {'num_train_epochs': 7, 'learning_rate': 5.2898091511494136e-05, 'alpha': 0.3140746400350408, 'temperature': 24}. Best is trial 3 with value: 0.8232351306281267.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-4_H-256_A-4 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1576' max='1576' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1576/1576 1:02:14, Epoch 8/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.320500</td>\n",
       "      <td>0.305639</td>\n",
       "      <td>0.718177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.306500</td>\n",
       "      <td>0.296624</td>\n",
       "      <td>0.749305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.301200</td>\n",
       "      <td>0.292987</td>\n",
       "      <td>0.760283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.299300</td>\n",
       "      <td>0.291159</td>\n",
       "      <td>0.765981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.297800</td>\n",
       "      <td>0.290268</td>\n",
       "      <td>0.769594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.296400</td>\n",
       "      <td>0.289715</td>\n",
       "      <td>0.772235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.295900</td>\n",
       "      <td>0.289577</td>\n",
       "      <td>0.771818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.296100</td>\n",
       "      <td>0.289440</td>\n",
       "      <td>0.772235</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-11 01:47:44,341] Trial 4 finished with value: 0.7722345747637577 and parameters: {'num_train_epochs': 8, 'learning_rate': 2.4373768682613424e-06, 'alpha': 0.4889425582875847, 'temperature': 30}. Best is trial 3 with value: 0.8232351306281267.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-4_H-256_A-4 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1379' max='1379' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1379/1379 54:22, Epoch 7/7]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.347200</td>\n",
       "      <td>0.326423</td>\n",
       "      <td>0.754030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.330400</td>\n",
       "      <td>0.319358</td>\n",
       "      <td>0.772790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.324700</td>\n",
       "      <td>0.316482</td>\n",
       "      <td>0.778210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.322400</td>\n",
       "      <td>0.314489</td>\n",
       "      <td>0.781267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.320400</td>\n",
       "      <td>0.313345</td>\n",
       "      <td>0.784742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.318500</td>\n",
       "      <td>0.312896</td>\n",
       "      <td>0.786409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.318000</td>\n",
       "      <td>0.312969</td>\n",
       "      <td>0.786965</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-11 02:42:08,753] Trial 5 pruned. \n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-4_H-256_A-4 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1379' max='1576' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1379/1576 54:14 < 07:45, 0.42 it/s, Epoch 7/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.125900</td>\n",
       "      <td>0.121610</td>\n",
       "      <td>0.753613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.123600</td>\n",
       "      <td>0.120967</td>\n",
       "      <td>0.774597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.122900</td>\n",
       "      <td>0.120667</td>\n",
       "      <td>0.784880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.122400</td>\n",
       "      <td>0.120360</td>\n",
       "      <td>0.780434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.122100</td>\n",
       "      <td>0.120237</td>\n",
       "      <td>0.788772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.121800</td>\n",
       "      <td>0.120142</td>\n",
       "      <td>0.789605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.121600</td>\n",
       "      <td>0.120099</td>\n",
       "      <td>0.793496</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-11 03:36:25,831] Trial 6 pruned. \n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-4_H-256_A-4 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='985' max='985' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [985/985 39:15, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.471500</td>\n",
       "      <td>0.425386</td>\n",
       "      <td>0.772235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.432600</td>\n",
       "      <td>0.411841</td>\n",
       "      <td>0.783352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.418800</td>\n",
       "      <td>0.406383</td>\n",
       "      <td>0.788772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.413500</td>\n",
       "      <td>0.402778</td>\n",
       "      <td>0.790717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.409800</td>\n",
       "      <td>0.401911</td>\n",
       "      <td>0.791829</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-11 04:15:53,731] Trial 7 finished with value: 0.791828793774319 and parameters: {'num_train_epochs': 5, 'learning_rate': 1.173971182301033e-05, 'alpha': 0.8077469751146494, 'temperature': 8}. Best is trial 3 with value: 0.8232351306281267.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-4_H-256_A-4 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='591' max='591' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [591/591 23:17, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.275000</td>\n",
       "      <td>0.262550</td>\n",
       "      <td>0.775292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.266200</td>\n",
       "      <td>0.259816</td>\n",
       "      <td>0.783213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.263300</td>\n",
       "      <td>0.259382</td>\n",
       "      <td>0.784047</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-11 04:39:16,732] Trial 8 finished with value: 0.7840466926070039 and parameters: {'num_train_epochs': 3, 'learning_rate': 1.4180834470145417e-05, 'alpha': 0.43662764362367845, 'temperature': 19}. Best is trial 3 with value: 0.8232351306281267.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-4_H-256_A-4 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1576' max='1576' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1576/1576 1:02:04, Epoch 8/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.377100</td>\n",
       "      <td>0.351038</td>\n",
       "      <td>0.776126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.356000</td>\n",
       "      <td>0.343978</td>\n",
       "      <td>0.789883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.347300</td>\n",
       "      <td>0.340755</td>\n",
       "      <td>0.797526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.342800</td>\n",
       "      <td>0.337214</td>\n",
       "      <td>0.803085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.339100</td>\n",
       "      <td>0.336575</td>\n",
       "      <td>0.806003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.335400</td>\n",
       "      <td>0.335240</td>\n",
       "      <td>0.810033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.333700</td>\n",
       "      <td>0.335258</td>\n",
       "      <td>0.809061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.333400</td>\n",
       "      <td>0.334870</td>\n",
       "      <td>0.811006</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-11 05:41:33,745] Trial 9 finished with value: 0.81100611450806 and parameters: {'num_train_epochs': 8, 'learning_rate': 1.3582277085236825e-05, 'alpha': 0.625656755680376, 'temperature': 10}. Best is trial 3 with value: 0.8232351306281267.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BestRun(run_id='3', objective=0.8232351306281267, hyperparameters={'num_train_epochs': 7, 'learning_rate': 5.2898091511494136e-05, 'alpha': 0.3140746400350408, 'temperature': 24}, run_summary=None)\n"
     ]
    }
   ],
   "source": [
    "def student_init():\n",
    "    return AutoModelForSequenceClassification.from_pretrained(\n",
    "        student_id,\n",
    "        num_labels=num_labels,\n",
    "        id2label=id2label,\n",
    "        label2id=label2id\n",
    "    )\n",
    "\n",
    "trainer = DistillationTrainer(\n",
    "    model_init=student_init,\n",
    "    args=training_args,\n",
    "    teacher_model=teacher_model,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "best_run = trainer.hyperparameter_search(\n",
    "    n_trials=10,\n",
    "    direction=\"maximize\",\n",
    "    hp_space=hp_space\n",
    ")\n",
    "\n",
    "print(best_run)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4bbf414f-dfa8-41bc-8a5c-63c22333602b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# overwrite initial hyperparameters with from the best_run\n",
    "for k,v in best_run.hyperparameters.items():\n",
    "    setattr(training_args, k, v)\n",
    "\n",
    "# Define a new repository to store our distilled model\n",
    "best_model_ckpt = \"tiny-bert-toxigen-best\"\n",
    "training_args.output_dir = best_model_ckpt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "01b3699e-6462-4f90-b02e-be5e4c42bd9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1379' max='1379' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1379/1379 54:57, Epoch 7/7]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.181900</td>\n",
       "      <td>0.192350</td>\n",
       "      <td>0.822679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.179100</td>\n",
       "      <td>0.192187</td>\n",
       "      <td>0.822262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.177200</td>\n",
       "      <td>0.195020</td>\n",
       "      <td>0.814341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.176100</td>\n",
       "      <td>0.193159</td>\n",
       "      <td>0.823930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.175600</td>\n",
       "      <td>0.193216</td>\n",
       "      <td>0.823374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.175200</td>\n",
       "      <td>0.193865</td>\n",
       "      <td>0.824208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.175900</td>\n",
       "      <td>0.193707</td>\n",
       "      <td>0.822957</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'https://huggingface.co/joseph10/tinybert-TG-HS-HX-parentpretrained/tree/main/'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a new Trainer with optimal parameters\n",
    "optimal_trainer = DistillationTrainer(\n",
    "    student_model,\n",
    "    training_args,\n",
    "    teacher_model=teacher_model,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "optimal_trainer.train()\n",
    "\n",
    "\n",
    "# save best model, metrics and create model card\n",
    "optimal_trainer.create_model_card(model_name=training_args.hub_model_id)\n",
    "optimal_trainer.push_to_hub()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44412f80-80a3-4025-9dac-9d4f7ecbc5f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
