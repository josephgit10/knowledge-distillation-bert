The file titled "Knowledge Distillation" contains code to perform knowledge distillation from a BERT model to a smaller, more efficient model like tinyBERT or distilBERT and fine-tune it on a dataset such as hate_speech 18. The tokenizer can be set to AutoTokenizer to select an appropriate tokenizer automatically. The max token length has been set to 256, wwhich can be increased to 512 depending upon the computational power available to you. Similarly, the n_trials and num_epochs can be increased to get better results with the downside of higher training time.
