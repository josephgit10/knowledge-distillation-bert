The file titled "Knowledge Distillation" contains code to perform knowledge distillation from a BERT model to a smaller, more efficient model like tinyBERT or distilBERT and fine-tune it on a dataset such as hate_speech 18.
